---
title: "Assignment 4"
author: "Santiago Rattenbach, Àngel Jiménez, Albert Salom"
date: "21/11/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries, include=FALSE}
# Load the required libraries, without showing warning messages
suppressWarnings({
  suppressPackageStartupMessages({
    library(ggplot2)
    library(GGally)
    library(cluster)
    library(factoextra)
    library(data.table)
    library(fpc)
    
  })
})
```

## The Data

```{r}
data <- read.csv("./penguindata.csv", header=TRUE, stringsAsFactors=TRUE)
str(data)
```

Si hacemos un primer vistazo, podemos observar como tenemos varios valores NAs que deberemos gestionar
posteriormente.

### Independent Variables

The dataset includes measurements taken for penguins in Palmer Archipelago. Variable information:
size (flipper length, body mass, bill dimensions), sex and year.

- **bill_length_mm:** a number denoting bill length (millimeters)
- **bill_depth_mm:** a number denoting bill depth (millimeters)
- **flipper_length_mm:** an integer denoting flipper length (millimeters)
- **body_mass_g:** an integer denoting body mass (grams)
- **sex:** a factor denoting penguin sex (female, male)
- **year:** an integer denoting the study year (2007, 2008, or 2009)

### Summary of the Data

Before start training the model, it is important to analyze each of the independent variables to understand
their values, distribution, and relationship with the target variable.

```{r}
summary(data)
```
We can remove the "x" column as it is not relevant for the analysis.
```{r}
data <- data[, -1]
```

Como podemos observar, tenemos varios valores NA en las variables bill_length_mm, bill_depth_mm, 
flipper_length_mm, body_mass_g y sex. Más tarde los trataremos.

### Missing Value Analysis

To find missing values in the dataset, we can use the `is.na()` function in R. 

```{r}
colSums(is.na(data))
```

Como se puede observar, tenemos varias columnas con valores NA. Al tratarse de pocos valores, podemos borrar 
tranquilamente sin que el modelo se vea afectado.

```{r}
data <- na.omit(data)
summary(data)
```

#### Numerical Variables

- **bill_length_mm:** Este dataset contiene la longitud del pico de los pingüinos en milimetros. Vemos que 
  la longitud del pico de los pinguinos varia entre 32.1 y 59.6 milimetros, con una media de 44mm. No parece 
  haber datos excesivamente inusuales

- **bill_depth_mm:**  La profundidad del pico de los pingüinos también viene en milimetros, en un rango entre 
  13.1mm y 21.5mm, con una media de 17.2mm. De nuevo, no hay ningún dato a resaltar.

- **flipper_length_mm:** La longitud de las aletas de los pinguinos del dataset varia entre 172mm y 231mm, 
  con una media de 201mm. Tampoco parece haber ningun valor atípico.

- **body_mass_g:** En el dataset, podemos observar que el peso de los pinguinos puede variar bastante, esto 
  podría deberse a la edad de los pinguinos, ya que los pinguinos más jóvenes suelen pesar menos. El peso
  de los pinguinos varia entre 2700g y 6300g, con una media de 4202g.

```{r}
# List of numerical variables:
numeric <- c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g')

# Plot the distribution of each numerical variable:
for (n in numeric) {
  print(
    ggplot(data, aes(x = !!sym(n))) +
      geom_histogram(fill = "lightblue", color = "white", bins = 30) +
      labs(title = paste("Distribution of", n), x = n, y = "Frecuency") +
      theme_minimal()
  )
}
```

A primera vista, parece ser que las variables numericas están algo centradas, asemejandose a una distribución
normal. De todas formas, posteriormente realizaremos un análisis de normalidad para confirmar esto.

#### Categorical

```{r}
# List of categorical variables:
categories <- c('year', 'sex')

for (var in categories) {
  print(
    ggplot(data, aes_string(x = var)) +
      geom_bar(fill = "coral1") +
      labs(title = paste("Distribution of", var), x = var, y = "Frecuency") +
      theme_minimal()
  )
}
```

- **sex:** Se puede observar que hay una cantidad similar de pinguinos machos y hembras en el dataset. 
  Esto es importante ya que si hubiera una gran diferencia entre los dos sexos, podría afectar a la 
  precisión del modelo.

- **year:** En el dataset, podemos observar que en cada año se ha hecho el estudio a un número similar de 
  pingüinos. Nuevamente, esto nos beneficiará a la hora de entrenar el modelo.

### Data Correlations

```{r}
data$year <- as.factor(data$year)

ggpairs(data,                 # Data frame
        columns = 1:6,        # Columns
        aes(color = year,     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

Como se puede observar, vemos que el año del estudio no parece tener una gran influencia en las variables
numéricas así como en el sexo de los pinguinos. Esto es importante ya que si hubiera una gran diferencia
entre los años, podría afectar a la precisión del modelo. Por tanto, podemos concluir que el año no es una
variable importante para el modelo y procedemos a eliminarla.

```{r}
data <- data[, -6]
summary(data)
```


#### Numerical Variables

```{r}
numeric_Corr <- data[, c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm',
                        'body_mass_g')]

ggcorr(numeric_Corr, label = TRUE)
```
 Como se puede observar, existe una gran correlación entre todas las variables numéricas, lo que podría 
 afectar a la precisión del modelo. Para solucionar esto, aplicaremos PCA para así poder reducir la 
 dimensionalidad de los datos.
 
## Model Building

### Convert to Numerical Variables

```{r}
# Convertir las variables categóricas en variables one hot encoding
data_Num <- model.matrix(~ . -1, data = data)

# Convertir el resultado a un data frame
data_Num <- as.data.frame(data_Num)
summary(data_Num)
```

### Scaling the data

Para poder aplicar el clustering de forma correcta, es necesario escalar los datos con una media de 0 y una
desviación estándar de 1. Esto hace que quitemos la barrera de la escala en diferentes variables. 

```{r}
data_Scaled <- scale(data_Num)
summary(data_Scaled)
```

## Clustering models

### PCA 

Para poder ver mejor los resultados, aplicaremos PCA para reducir la dimensionalidad de los datos.

```{r}
data_Scaled.pca<- prcomp(data_Num, center=TRUE, scale=TRUE)
summary(data_Scaled.pca)
```

```{r}
plot(data_Scaled.pca, type="l")
```

Como se puede ver, las 2 componentes principales explican alrededor del 85% de la varianza de los datos. Esto nos
permite reducir la dimensionalidad de los datos sin perder mucha información. Además, a la hora de mostrar
los resultados, podremos visualizarlos en un espacio de 2 dimensiones.

```{r}
data_PCA <- as.data.frame(data_Scaled.pca$x[,1:2])
ggplot(data=data_PCA, aes(x=PC1, y=PC2, color=data$sex)) + geom_point() +
  labs(title="PCA", x="PC1", y="PC2")
```

### Deciding the number of clusters

Una manera de decidir el número de clusters es mediante el silhouette method. Este método nos permite 
determinar el número de clusters que mejor se ajusta a los datos. Para ello, es importante que la puntación	
sea lo más cercana a 1 posible.

Antes de aplicar el silhouette method, es importante ver de manera visual el número de clusters que mejor se
ajusta a los datos. Para ello, aplicaremos el método de "elbow plot".

```{r}
set.seed(123)
# Compute and plot wss for k = 2 to k = 15
k.max <- 15 # Maximal number of clusters
wss <- sapply(1:k.max, 
              function(k){kmeans(data_Scaled, k)$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
#abline(v = 3, lty =2)
```

Como se puede observar, el rango de 3 a 5 clusters parece ser el más adecuado ya que a partir del 5, apenas 
hay bajado incluso una anormal subida en 6. Para confirmar esto, aplicaremos el silhouette method.

```{r}
cl.kmeans <-kmeans(data_Scaled, 3, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

Como se puede observar, vemos como los 2 primeros clusters tienen una puntuación bastante alta, mientras que 
el tercero tiene una puntuación bastante baja. Esto nos indica que el número de clusters no es el adecuado.

```{r}
cl.kmeans <-kmeans(data_Scaled, 4, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

En este caso, vemos como el silhouette method nos indica que el número de clusters es el adecuado, ya que
una puntuación de media de 0.77, lo cual es bastante bueno. Igualmente, probaremos con 5 clusters para
confirmar que el número de clusters es el adecuado.

```{r}
cl.kmeans <-kmeans(data_Scaled, 5, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

Con 5 clusters vemos como la puntuación no mejora, por lo que es mejor quedarnos con 4 clusters.

## Partitional Clustering

### Clustering using K-means 

```{r}
# Compute the k-means clustering algorithm
kmeans_model <- kmeans(data_Scaled, centers = 4, nstart = 20)
data_PCA <- as.data.table(data_PCA)
cl.kmeans2 <- data_PCA[, km.Clusters := kmeans_model$cluster]

ggplot(cl.kmeans2, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(kmeans_model$cluster)), size = 2, alpha = 0.3) +
theme_minimal()
```

Vemos que los clusters quedan bastante definidos, lo cual es una buena señal. Además, no hay ningún outlier
que perjudique a los propios clusters. Veamos si se pueden sacar conclusiones con algunas variables.

```{r}
data2 <- as.data.table(data)
data2[, km.Clusters := kmeans_model$cluster]
ggplot(data2, aes(x = sex, y = body_mass_g)) +
geom_point(aes(colour = factor(kmeans_model$cluster)), size = 2, alpha = 0.3) +
theme_minimal()
```
 
En este caso, podemos indicar como a las hembras se les asigna el cluster 1 o 4, mientras que a los machos
se les asigna el cluster 2 o 3. Por tanto, el factor del sexo es un factor importante a la hora diferenciar
las muestras. Además se puede apreciar una tendencia ya que el clúster 1 forman parte del grupo de hembras
más pesadas, mientras que el clúster 4 forman parte del grupo de hembras más ligeras. En el caso de los
machos pasa igual, siendo el clúster 2 el que forman parte de los machos más pesados y el clúster 3 los
machos más ligeros.

Observemos a ver si se pueden apreciar mas tendencias con otras variables.

```{r}
ggpairs(data2,        # Columns
        aes(color = factor(kmeans_model$cluster),     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

En este gráfico se puede sacar bastante información. De primeras, confirmamos lo dicho anteriormente, que
el cluster 1 y 4 son hembras y el cluster 2 y 3 son machos. Por tanto, nos fijaremos en como se relacionan
las otras variables con el sexo de los pingüinos. 

Si nos fijamos en la longitud de la aleta, podemos observar como los clusters 1 y 2, los que pesan mas, 
son los que tienen unalongitud de aleta más larga, mientras que los clusters 3 y 4 son los que tienen una 
longitud de aleta más corta. Podría ayudarnos a diferenciar entre los machos y hembras adultos y los jóvenes.


### Clustering around K-Medoids (PAM)

Ahora vamos a probar con este algoritmo, usando 4 clusters, ya que anteriormente hemos visto que es lo más adecuado.

Probamos primero con la distancia euclidea:

```{r}
distEuclidea<-dist(data_Scaled, method="euclidean")
pam_Euclidean <- pam(distEuclidea, k = 4)
pam_Euclidean
```
```{r}
clusplot(pam_Euclidean)
```

Vamos a ver ahora que obtenemos si usamos la distancia de Manhattan:

```{r}
distManhattan<-dist(data_Scaled, method="manhattan")
# Compute the PAM clustering algorithm
pam_Manhattan <- pam(distManhattan, k = 4)
pam_Manhattan
```

```{r}
clusplot(pam_Manhattan)
```

Como podemos observar, con la distancia de Manhattan los resultados son mucho peores.

Ahora procedemos a aplicar la libreria pamk(), que nos estima el número de clusters óptimo.

```{r}
pamk <- pamk(distEuclidea)
pamk$nc # Nos muestra el número de clusters óptimo
```

Como podemos apreciar, efectivamente el número de clústers óptimo es el que nosotros habíamos ya definido.

### Clustering Density-based

```{r}
library(fpc)
dbscan.results <- dbscan(data_Scaled, eps=0.35, MinPts=5) 
dbscan.results
```

```{r}
plot(dbscan.results, data_PCA[, 1:2])
```

Como podemos observar hay demasiados clusters y muchos outliers, que es lo que nos indica la columna de "0", así que probaremos otros parámetros

```{r}
dbscan.results2 <- dbscan(data_Scaled, eps=0.75, MinPts=20) 
dbscan.results2
```
```{r}
plot(dbscan.results2, data_PCA[, 1:2])
```

Sigue habiendo muchos outliers, vamos a intentar mejorarlo

```{r}
dbscan.results3 <- dbscan(data_Scaled, eps=0.85, MinPts=5) 
dbscan.results3
```
```{r}
plot(dbscan.results3, data_PCA[, 1:2])
```

```{r}
plotcluster(data_PCA[, 1:2], dbscan.results3$cluster)
```

Como podemos observar, el número de clusters es el adecuado, ya que apenas hay outliers y los clusters están
bien definidos.

## Hierarchical Clustering

Primero vamos a normalizar los datos para poder aplicar el clustering de forma correcta.

```{r}
# Create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Normalize the data
data_Normalized <- as.data.frame(lapply(data[1:4], normalize))
data_Normalized <-cbind(data_Normalized, data[,5])
summary(data_Normalized)
```
```{r}
# Calcular la distancia euclidiana solo para las columnas numéricas
distEuclidea <- dist(data_Normalized[, 1:4], method = "euclidean")
```

```{r}
hclust_single <- hclust(distEuclidea, method = "single")
plot(hclust_single, main = "Dendrograma de Clustering - Single", hang = -1)
```

Como podemos observar, tenemos demasiadas muestras para poder analizar correctamente y se satura el gráfico, vamos a seleccionar un menor número de muestras de forma aleatoria.

```{r}
set.seed(2837)

idx <- sample(1:dim(data_Normalized)[1], 60)  # Seleccionar 40 muestras aleatorias
data_Sample <- data_Normalized[idx, ]  

distEuclidea <- dist(data_Sample[, 1:4], method = "euclidean")
```

```{r}
hclust_single <- hclust(distEuclidea, method = "single")
plot(hclust_single, main = "Dendrograma de Clustering - Single", cex = 0.6, hang = -1)
```

Ahora procederemos a cortar en 4 clústers, como pensamos que se separan mejor los datos.

```{r}
plot(hclust_single, main = "Dendrograma de Clustering - Single", cex = 0.6, hang = -1) 

rect.hclust(hclust_single, k=4)
```
```{r}
plot(data_Sample$x[, 1:2], col = groups)
text(x = data_Sample$x[, 1:2], labels=data_Sample$idx, pos = 3, cex = 0.5, col = groups)
```


```{r}
groups <- cutree(hclust_single, k = 4)

# Añadir los clusters al dataset
data_Sample <- as.data.table(data_Sample)
data_Sample[, hc.Clusters := clusters_hc]

# Visualizar los resultados en un gráfico
ggplot(data_Sample, aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(aes(colour = factor(hc.Clusters)), size = 3, alpha = 0.8) +
  theme_minimal() +
  labs(title = "Clustering Jerárquico - 4 Clústeres", x = "Bill Length (mm)", y = "Body Mass (g)")
```

No pinta bien :v


```{r}
hclust_avg <- hclust(distEuclidea, method = "average")
plot(hclust_avg, main = "Dendrograma de Clustering - Average", hang = -1)
```

```{r}
hclust_complete <- hclust(distEuclidea, method = "complete")
plot(hclust_complete, main = "Dendrograma de Clustering - Complete", hang = -1)
```

```{r}

```

## Conclusion



### Choosing the best model



### Our Learnings



### References

- [R Documentation] (https://www.rdocumentation.org/): Used to understand some of the functions and packages
  used in our analysis.
- [ChatGPT] (chatgpt.com): Used for help in making sense of the data, translation of some text originally
  written in Spanish and removal of certain warnings generated by library functions.
- [RMD files from class]: Used as a reference for the structure of the document and as examples of
  model implementation.
