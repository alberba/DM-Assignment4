---
title: "Assignment 4"
author: "Santiago Rattenbach, Àngel Jiménez, Albert Salom"
date: "21/11/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries, include=FALSE}
# Load the required libraries, without showing warning messages
suppressWarnings({
  suppressPackageStartupMessages({
    library(ggplot2)
    library(GGally)
    library(cluster)
    library(factoextra)
    library(data.table)
    library(fpc)
    
  })
})
```

## The Data

```{r}
data <- read.csv("./penguindata.csv", header=TRUE, stringsAsFactors=TRUE)
str(data)
```

If we take a first look, we can observe that we have several NA values that we will need to handle later.

### Independent Variables

The dataset includes measurements taken for penguins in Palmer Archipelago. Variable information:
size (flipper length, body mass, bill dimensions), sex and year.

- **bill_length_mm:** a number denoting bill length (millimeters)
- **bill_depth_mm:** a number denoting bill depth (millimeters)
- **flipper_length_mm:** an integer denoting flipper length (millimeters)
- **body_mass_g:** an integer denoting body mass (grams)
- **sex:** a factor denoting penguin sex (female, male)
- **year:** an integer denoting the study year (2007, 2008, or 2009)

### Summary of the Data

Before start training the model, it is important to analyze each of the independent variables to understand
their values, distribution, and relationship with the target variable.

```{r}
summary(data)
```

We can remove the "X" column as it is not relevant for the analysis.

```{r}
# Remove the first column
data <- data[, -1]
```

### Missing Value Analysis

To find missing values in the dataset, we can use the `is.na()` function in R. 

```{r}
colSums(is.na(data))
```

As we can see, we have several columns with NA values. Since there are only a few values, we can safely 
remove them without affecting the model.

```{r}
data <- na.omit(data)
summary(data)
```

Now we can see that there are no NA values in the dataset, and therefore we can proceed with our analysis
correctly.

#### Numerical Variables

- **bill_length_mm:** This dataset contains the bill length of penguins in millimeters. We see that it varies 
  between 32.1 and 59.6 millimeters, with an average of 44mm. There do not appear to be any excessively 
  unusual data points.

- **bill_depth_mm:** The bill depth of penguins is also in millimeters, ranging from 13.1mm to 21.5mm, with 
  an average of 17.2mm. Again, there are no notable data points.

- **flipper_length_mm:** The flipper length of the penguins in the dataset varies between 172mm and 231mm, 
  with an average of 201mm. There do not appear to be any outliers.

- **body_mass_g:** In the dataset, we can observe that the weight of the penguins can vary significantly, 
  which could be due to the age of the penguins, as younger penguins tend to weigh less. The weight of the 
  penguins varies between 2700g and 6300g, with an average of 4202g.

```{r}
# List of numerical variables:
numeric <- c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g')

# Plot the distribution of each numerical variable:
for (n in numeric) {
  print(
    ggplot(data, aes(x = !!sym(n))) +
      geom_histogram(fill = "lightblue", color = "white", bins = 30) +
      labs(title = paste("Distribution of", n), x = n, y = "Frecuency") +
      theme_minimal()
  )
}
```

At first glance, it seems that the numerical variables are somewhat centered, resembling a normal distribution. 

#### Categorical

```{r}
# List of categorical variables:
categories <- c('year', 'sex')

for (var in categories) {
  print(
    ggplot(data, aes_string(x = var)) +
      geom_bar(fill = "coral1") +
      labs(title = paste("Distribution of", var), x = var, y = "Frecuency") +
      theme_minimal()
  )
}
```

- **sex:** We can observe that there is a similar number of male and female penguins in the dataset. 
  This is important because if there were a significant difference between the two sexes, it could affect 
  the accuracy of the model.

- **year:** In the dataset, we can observe that each year a similar number of penguins were studied. 


### Data Correlations

```{r}
data$year <- as.factor(data$year)

ggpairs(data,                 # Data frame
        columns = 1:6,        # Columns
        aes(color = year,     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

As we can see, the study year does not seem to have a significant influence on the numerical variables 
or the sex of the penguins. This is important because if there were a significant difference between 
the years, it could affect the accuracy of the model. 

Therefore, we can conclude that the year is not an important variable for the model and proceed to remove it.

```{r}
# Remove the year column
data <- data[, -6]
summary(data)
```

#### Numerical Variables

```{r}
numeric_Corr <- data[, c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm',
                        'body_mass_g')]

ggcorr(numeric_Corr, label = TRUE)
```
 
As we can see, there is a high correlation between all the numerical variables, which could affect 
the accuracy of the model. To solve this, we will apply PCA to reduce the dimensionality of the data.
 
## Model Building

### Convert to Numerical Variables

```{r}
# Convert categorical variables to one hot encoding
data_Num <- model.matrix(~ . -1, data = data)

# Convert to data frame
data_Num <- as.data.frame(data_Num)
summary(data_Num)
```

### Scaling the data

To correctly apply clustering, it is necessary to scale the data with a mean of 0 
and a standard deviation of 1. This removes the scale barrier in different variables.

```{r}	
data_Scaled <- scale(data_Num)
summary(data_Scaled)
```

## Clustering Models

### Applying PCA 

To better visualize the results, we will apply PCA to reduce the dimensionality of the data.

```{r}
data_Scaled.pca<- prcomp(data_Num, center=TRUE, scale=TRUE)
summary(data_Scaled.pca)
```

```{r}
plot(data_Scaled.pca, type="l")
```

As we can see, both principal components explain around 85% of the variance of the data. This allows us to
reduce the dimensionality of the data without losing much information. Furthermore, when displaying the results,
we can visualize them in a 2-dimensional space.

```{r}
data_PCA <- as.data.frame(data_Scaled.pca$x[,1:2])
ggplot(data=data_PCA, aes(x=PC1, y=PC2, color=data$sex)) + geom_point() +
       labs(title="PCA", x="PC1", y="PC2")
```

### Deciding the number of clusters

One way we can decide the number of clusters is through the silhouette method. This method allows us to
determine the number of clusters that best fits the data. To do this, it is important that the score is as
close to 1 as possible.
the data. For this, we will apply the "elbow plot" method.Before applying the silhouette method, it is important to visually see the number of clusters that best fits the data. 
To do this, we will apply the "elbow plot" method.

```{r}
set.seed(123)
# Compute and plot wss for k = 2 to k = 15
k.max <- 15 # Maximal number of clusters
wss <- sapply(1:k.max, 
              function(k){kmeans(data_Scaled, k)$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```

As we can see, the range of 3 to 5 clusters seems to be the most appropriate since from 5 onwards, there is
hardly any decrease, even an abnormal increase at 6. To confirm this, we will apply the silhouette method.As we can see, the range of 3 to 5 clusters seems to be the most suitable since from 5 onwards, there is barely any decrease 
and even an abnormal increase at 6. To confirm this, we will apply the silhouette method.

```{r}
cl.kmeans <-kmeans(data_Scaled, 3, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

It can be seen that the first 2 clusters have a fairly high score, while the third one has a fairly low
score. This indicates that the number of clusters is not correct.
As we can see, the first two clusters have a fairly high score, while the third has a fairly low score. This indicates that 
the number of clusters is not adequate.

```{r}
cl.kmeans <-kmeans(data_Scaled, 4, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

In this case, we see how the silhouette method indicates that the number of clusters is correct, since it has
a mean score of 0.77, which is quite good. Either way, we will also try with 5 clusters to confirm that the
number of clusters is correct.In this case, we see that the silhouette method indicates that the number of clusters is adequate, as the average score is 0.77, 
which is quite good. However, we will try with 5 clusters to confirm that the number of clusters is appropriate.

```{r}
cl.kmeans <-kmeans(data_Scaled, 5, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

With 5 clusters, we see how the score does not improve, so it is better to stick with 4 clusters.With 5 clusters we see that the score does not improve, so it is better to stick with 4 clusters.

### Partitional Clustering

A clustering method that divides data into non-overlapping clusters, where each data point belongs to one cluster (e.g., k-means).

#### Clustering using K-means 

```{r}
kmeans_model <- kmeans(data_Scaled, centers = 4, nstart = 20)
data_PCA <- as.data.table(data_PCA)
cl.kmeans2 <- data_PCA[, km.Clusters := kmeans_model$cluster]

ggplot(cl.kmeans2, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(kmeans_model$cluster)), size = 2, alpha = 0.3) +
theme_minimal()
```

As we can see, the clusters are quite well defined, which is a good sign. In addition, there are no outliers
that harm the clusters. Let's see if we can draw conclusions with some variables.We can see that the clusters are well defined, which is a good sign. Additionally, there are no outliers that could negatively
affect the clusters. Let's see if we can draw conclusions with some variables.

```{r}
data2 <- as.data.table(data)
data2[, km.Clusters := kmeans_model$cluster]
ggplot(data2, aes(x = sex, y = body_mass_g)) +
geom_point(aes(colour = factor(kmeans_model$cluster)), size = 2, alpha = 0.3) +
theme_minimal()
```

In this case, we can point out that females get assigned to clusters 1 or 4, while males get assigned to
clusters 2 or 3. Therefore, the sex variable is an important factor when differentiating the samples. In
addition, we can see a trend, as cluster 1 groups the heavier females, while cluster 4 groups the lighter
females. The same can be noted with the male samples, as the heavier males are grouped in cluster 2 and
lighter males in cluster 3.

Let's see if we can observe more trends with other variables.

```{r}
ggpairs(data2,        # Columns
        aes(color = factor(kmeans_model$cluster),     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

We can extract quite a bit of information from this plot. First, we confirm what we said earlier, that
clusters 1 and 4 are composed of female samples while clusters 2 and 3 are composed by male samples.
Therefore, we will focus on how the other variables relate.

If we look at the flipper length, we can see that clusters 1 and 2, the heaviest ones, are the ones with
the longer flipper length, while clusters 3 and 4 are the ones with the shorter flipper length. This could
help us differentiate between male, female adults and young penguins for example.

If we look at the bill depth, however, we can see that the trend is the opposite. According to our
hypothesis, this should mean that young penguins have a higher bill depth, which should not not the case.
Therefore, the clusters do not seem to split by age and sex. Therefore, it could be that the dataset splits
by penguin species. To test this, we will modify the dataset later.

### Clustering around K-Medoids (PAM)

PAM (Partitioning Around Medoids) is a clustering algorithm that is similar to k-means, but instead of using
the mean of the cluster, it uses the medoid. The medoid is the most centrally located point in the cluster.
We will now use the pamk function from the fpc package to determine the optimal number of clusters.

```{r}
distEuclidea <- dist(data_Scaled, method="euclidean")
#distEuclidea <- dist(data_Scaled, method="manhattan")
pamk <- pamk(distEuclidea)
pamk$nc # Nos muestra el número de clusters óptimo
```

As we can see, the optimal number of clusters is indeed 4, which is the same as we decided with k-means.

Now we will try this algorithm using 4 clusters, as we decided earlier. We will first try with the Euclidean distance.

```{r}
pam_Euclidean <- pam(distEuclidea, k = 4)
pam_Euclidean
```

```{r}
clusplot(pam_Euclidean)
```

We see that the clusters are defined just as well as with k-means. This is because the Euclidean distance 
is the most suitable for continuous or mixed values (continuous and categorical). This is our case.

Let's see now what we get if we use the Manhattan distance.

```{r}
distManhattan <- dist(data_Scaled, method="manhattan")
pam_Manhattan <- pam(distManhattan, k = 4)
pam_Manhattan
```

```{r}
clusplot(pam_Manhattan)
```

As we can see, with the Manhattan distance, the results are much worse. We can observe how the samples 
of the clusters are mixed, which is not good. Therefore, the Euclidean distance seems to be better.

### Clustering Density-based

For density-based clustering, we will use the DBSCAN algorithm. The parameters are initially random until 
we find the parameters that best fit the data.

```{r}
library(fpc)
dbscan.results <- dbscan(data_Scaled, eps=0.35, MinPts=5) 
dbscan.results
```

As we can see, we get 7 clusters, which is quite different from the other models. Let's see how they are distributed.

```{r}
plot(dbscan.results, data_PCA[, 1:2])
```

As we can see, there are too many clusters and many outliers, as indicated by the "0" column, 
so these parameters are not suitable. Let's try with different parameters.

```{r}
dbscan.results2 <- dbscan(data_Scaled, eps=0.75, MinPts=20) 
dbscan.results2
```

In this case, the data has been reduced to 5 clusters, but there are still too many. 
Additionally, there are still many outliers, specifically 70. Let's see the plot.

```{r}
plot(dbscan.results2, data_PCA[, 1:2])
```

There are still many outliers, and some clusters are mixed. Therefore, let's try with less restrictive parameters.

```{r}
dbscan.results3 <- dbscan(data_Scaled, eps=0.85, MinPts=5) 
dbscan.results3
```

In this case, the data has been reduced to 4 clusters, which is the number of clusters from the other models. 
Additionally, it has 4 outliers, which is an acceptable number. Let's see the plot.

```{r}
plot(dbscan.results3, data_PCA[, 1:2])
```

As we can see, the clusters are almost as well defined as with k-means and PAM. Additionally, there are hardly any outliers,
which is a good sign.

### Hierarchical Clustering

A method that builds a tree-like structure (dendrogram) to group data. This dendrogram, shows the merging or 
splitting process and helps decide the optimal number of clusters.


#### Normalizing the data

First, we will normalize the data to correctly apply the clustering.

```{r}
# Create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Normalize the data
data_Normalized <- as.data.frame(lapply(data_Num, normalize))
summary(data_Normalized)
```

Additionally, we will use the Euclidean distance to correctly apply the clustering, 
as we have discovered that it is the best for our data.

```{r}
# Calcular la distancia euclidiea
distEuclidea <- dist(data_Normalized[, 1:6], method = "euclidean")
#distEuclidea <- dist(data_Normalized[, 1:6], method = "manhattan")
```

### Single Linkage

We will start with the single linkage hierarchical clustering method, which merges the closest clusters at each step. 
The distance between clusters is calculated as the distance between the closest points of each cluster.

```{r}
hclust_single <- hclust(distEuclidea, method = "single")
plot(hclust_single, main = "Clustering dendrogram - Single", hang = -1)
```

As we can see, we have too many samples to analyze correctly, and the graph becomes saturated. 
Similarly, given the context of our dataset, it does not make sense to create a sample with a smaller dataset, 
since without having any dependent variable, it is not useful to see that an individual belongs to one cluster or another.

Let's see how many clusters the dataset should be divided into.

```{r}
heigth_single<-hclust_single$height # height values
heigth_single
```

From the height of the clusters, we will determine the number of clusters. The highest "jump" will indicate the optimal 
number of clusters. To do this, we will see the difference between the highest heights.

```{r}
height <- c(0, heigth_single[-length(heigth_single)]) # vector that has to be substracted from height.cl
max(round(heigth_single-height,3)) # the largest increase
```

This value represents the largest "jump" in the distance between clusters throughout the clustering process. 
In the context of hierarchical clustering, a large increase in height indicates that two very distinct clusters 
(in terms of their characteristics) have been merged, which can be a good point to cut the dendrogram.

```{r}
which.max(round(heigth_single-height,3)) # the step of the largest increase
```

As we can see with this last value, the largest jump occurs in step 332, so it is determined that the optimal
number of clusters is 2.

We know, however, that these two clusters are probably the result of dividing the penguins between male and
female, which isn't useful at all as that classification is obvious and we are trying to divide the penguins
in a more meaningful way, such as species. Because of this, we are going to try again later but we will
remove the sex column from the dataset, in order to try to divide the penguins in a more meaningful way.

```{r}
groups <- cutree(hclust_single, k = 2)

plot(hclust_single, main = "Clustering dendrogram - Single", cex = 0.6, hang = -1)
rect.hclust(hclust_single, k=2)
```

Let's see how the clusters are distributed using PCA.

```{r}
# Add the clusters from the original dataframe to the PCA dataframe
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

### Complete Linkage

```{r}
hclust_complete <- hclust(distEuclidea, method = "complete")
plot(hclust_complete, main = "Clustering dendrogram - Complete", hang = -1)
heigth_complete<-hclust_complete$height
height <- c(0, heigth_complete[-length(heigth_complete)]) # vector that has to be substracted from height.cl
round(heigth_complete-height,3)
max(round(heigth_complete-height,3))
which.max(round(heigth_complete-height,3)) # the step of the largest increase
```

In this case, the largest jump occurs in the last step, so it determines that the number of clusters is 2 again.

```{r}
groups <- cutree(hclust_complete, k = 2)
# Add the clusters to the PCA dataframe from the original dataframe
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

We see that this distribution into 2 clusters does not provide useful information as we indicated earlier.

### Average Linkage

```{r}
hclust_avg <- hclust(distEuclidea, method = "average")
plot(hclust_avg, main = "Clustering dendrogram - Average", hang = -1)
heigth_average <- hclust_avg$height
height <- c(0, heigth_average[-length(heigth_average)]) # vector that has to be substracted from height.cl
round(heigth_average-height,3)
max(round(heigth_average-height,3))
which.max(round(heigth_average-height,3)) # the step of the largest increase
```

It is still 2 clusters, which does not provide additional information. Let's at least see how the clusters are distributed.

```{r}
groups <- cutree(hclust_avg, k = 2)

# Add the clusters from the original dataframeto the PCA dataframe
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

As we can see, we end up with very similar results so we can assume that the number of recommended clusters
will be two again. We will try with the complete method now.

### Centroid Linkage

```{r}
hclust_centroid <- hclust(distEuclidea, method = "centroid")
plot(hclust_centroid, main = "Clustering dendrogram - Centroid", hang = -1)
heigth_centroid <- hclust_centroid$height
height <- c(0, heigth_centroid[-length(heigth_centroid)]) # vector that has to be substracted from height.cl
round(heigth_centroid-height,3)
max(round(heigth_centroid-height,3))
which.max(round(heigth_centroid-height,3)) # the step of the largest increase
```

In this case, the largest jump occurs in step 332, so it is determined that the optimal number of clusters
seems to be 2 using this method.

```{r}
groups <- cutree(hclust_centroid, k = 2)
# Add the clusters to the PCA dataframe from the original dataframe
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

We can see that it has formed a tree almost identical to the average one, so the final clusters are the same.

### Ward Linkage

```{r}
hclust_ward <- hclust(distEuclidea, method = "ward.D2")
plot(hclust_ward, main = "Clustering dendrogram - Ward", hang = -1)
heigth_ward <- hclust_ward$height
height <- c(0, heigth_ward[-length(heigth_ward)]) # vector that has to be substracted from height.cl
round(heigth_ward-height,3)
max(round(heigth_ward-height,3))
which.max(round(heigth_ward-height,3)) # the step of the largest increase
```

Using Ward's method, we can see that the largest jump occurs at step 332, which determines that the optimal 
number of clusters is 2 again.

```{r}
groups <- cutree(hclust_ward, k = 2)
# Add the clusters to the PCA dataframe from the original dataframe
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

Again, we see that nothing changes.

## Clustering with NO_SEX

```{r}
dataNoSex <- data[1:4]
dataNoSex_Scaled <- scale(dataNoSex)
dataNoSex_Scaled.pca<- prcomp(dataNoSex, center=TRUE, scale=TRUE)
summary(dataNoSex_Scaled.pca)
```

We see that the 2 principal components explain around 89% of the variance in the data. This allows us to 
reduce the dimensionality to 2 without losing much information.

```{r}
data_PCA2 <- as.data.frame(dataNoSex_Scaled.pca$x[,1:2])
data_PCA2 <- as.data.table(data_PCA2)
```

### Partitional Clustering

First, let's see what the optimal number of clusters would be in this case.

```{r}
set.seed(123)
# Compute and plot wss for k = 2 to k = 15
k.max <- 15 # Maximal number of clusters
wss <- sapply(1:k.max, 
              function(k){kmeans(dataNoSex_Scaled, k)$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
```

As we can see, the range of 2 to 4 clusters seems to be the most appropriate since from 4 onwards, there is 
hardly any decrease, even an abnormal increase at 6. To confirm this, we will apply the silhouette method.

```{r}
cl.kmeans <-kmeans(dataNoSex_Scaled, 2, nstart = 20)
dis <- dist(dataNoSex_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

As we can see, the 2 clusters have a score close to or higher than 0.7, and the average silhouette score is 0.73,
which is quite good. We will also try with 3 clusters to see if it improves.

```{r}
cl.kmeans <-kmeans(dataNoSex_Scaled, 3, nstart = 20)
dis <- dist(dataNoSex_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

In this case, we see that the silhouette score has slightly decreased, indicating that it is better to stick with 2 clusters instead of 3.

```{r}
cl.kmeans <-kmeans(dataNoSex_Scaled, 4, nstart = 20)
dis <- dist(dataNoSex_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

With 4 clusters, we see that the score does not improve, so it is better to stick with 2 clusters.

#### K-means

```{r}
# Compute the k-means clustering algorithm
kmeansNoSex_model <- kmeans(dataNoSex_Scaled, centers = 2, nstart = 20)
data_PCA2 <- as.data.table(data_PCA2)
cl.kmeans3 <- data_PCA2[, km.Clusters := kmeans_model$cluster]

ggplot(cl.kmeans3, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(kmeansNoSex_model$cluster)), size = 2, alpha = 0.3) +
theme_minimal()
```

We can see that the clusters are quite well defined.

#### PAM

```{r}
distEuclidea <- dist(dataNoSex_Scaled, method="euclidean")
#distEuclidea <- dist(data_Scaled, method="manhattan")
pamk <- pamk(distEuclidea)
pamk$nc # Shows the best number of clusters
```

As we can see, the optimal number of clusters is indeed 2, which is the same as we decided with k-means.

Now we will try this algorithm using 2 clusters, as we decided earlier. We will first try with the Euclidean distance.

```{r}
pam_Euclidean <- pam(distEuclidea, k = 2)
pam_Euclidean
```

```{r}
clusplot(pam_Euclidean)
```

We see that the clusters are defined just as well as with k-means, as before.

Let's see now what we get if we use the Manhattan distance.

```{r}
distManhattan <- dist(data_Scaled, method="manhattan")
pam_Manhattan <- pam(distManhattan, k = 2)
pam_Manhattan
```

```{r}
clusplot(pam_Manhattan)
```

As we can see, with the Manhattan distance, the results are much worse having just a 21.55% of the explanation with those components. 
So, as what happened before, we will apply just the Euclidean distance.

### Hierarchical Clustering

Now we will visualize the clusters using fviz_dend, which allows us to visualize the clusters in a more
attractive way, but first we need to know the optimal number of clusters.

#### Single Linkage

```{r}
hclust_single <- hclust(distEuclidea, method = "single")

heigth_single<-hclust_single$height # height values
height <- c(0, heigth_single[-length(heigth_single)]) # vector that has to be substracted from height.cl

round(heigth_single-height,3)
which.max(round(heigth_single-height,3)) # the step of the largest increase
```

As we can see with this last value, the largest jump occurs in step 331, so it is determined that the optimal 
number of clusters is 3. Therefore, we will cut into 3 clusters.

```{r}
fviz_dend(hclust_single, k = 3, # cut in four groups
         cex = 0.6, # label size
         k_colors = c("#2E9FDF", "#00AFBB", "#E7B800"),
         color_labels_by_k = TRUE
)
```

Now we add them to the dataset and visualize the clusters with PCA.

```{r}
groups <- cutree(hclust_single, k = 3)

cl.hclust <- data_PCA2[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

As we saw earlier, it has left a cluster with a single sample. Therefore, it does not seem to be a good model.

#### Average Linkage

```{r}
hclust_average <- hclust(distEuclidea, method = "average")
plot(hclust_average, main = "Clustering dendrogram - Average", hang = -1)
```

```{r}
heigth_average<-hclust_average$height # height values
height <- c(0, heigth_average[-length(heigth_average)]) # vector that has to be substracted from height.cl

which.max(round(heigth_average-height,3)) # the step of the largest increase
```

Let's visualize it using fviz_cluster(), grouping the data into 2 clusters.

```{r}
groups <- cutree(hclust_average, k = 2)

fviz_cluster(list(data = dataNoSex, cluster = groups),
        palette = c("#D55E00","#0072B2"),
        ellipse.type = "convex", # Concentration ellipse
        repel = TRUE, # Avoid label overplotting (slow)
        show.clust.cent = FALSE, 
        ggtheme = theme_minimal()
)
```

We prepare it to visualize it with PCA, adding the data to the dataset.

Visualizing the clusters with PCA.

```{r}
# Add the clusters from the original dataframeto the PCA dataframe
cl.hclust <- data_PCA2[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

It can be seen that clusters end up divided in a similar way to the k-means and PAM, which is a good sign.

## Interpretation of the Results

From the model that best defines the clusters, we can try to hypothesize with the results obtained.
To do this, we have the ggpairs, which will help us visualize the relationships between the variables and
the clusters.

```{r}
data3 <- as.data.table(data)
data3[, km.Clusters := kmeansNoSex_model$cluster]
ggpairs(data3,        # Columns
        aes(color = factor(kmeansNoSex_model$cluster),     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

It can be seen that removing the sex variable from the dataset has not provided additional information, as in
the clusters of the first version of the dataset we saw that each sex had two clusters. Now, with the new
version of the dataset, it seems that the two clusters of each sex are onne in the same, which does not
provide additional information.

```{r}	
ggpairs(data2,        # Columns
        aes(color = factor(kmeans_model$cluster),     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

On the other hand, as we said before, we initially came to the conclusion that the clusters could be grouping
the penguins according to their age (adult penguins and young penguins). This seems to make sense if we look
at the variables of weight and flipper length, but if we look at the bill depth, it does not make sense, as,
according to the assumption, young penguins should have a deeper bill, which would mean that it decreases as
the penguins grow up, which is simply illogical.

It is possible that, instead of being grouped by age or sex, the penguins are being grouped by species. This
could be possible, as different species of the same animal may have differences in size and weight of certain
body parts (or the body as a whole). In our case, it could be possible that one of the two species of
penguins is larger than the other and, consequently, has a longer flipper. Regarding the bill, the
lighter penguins are prone to having a deeper bill and their length is shorter and can fall within a larger
range of values compared to the heavier penguins.

## Choosing the best model

We don't see fit to choose the best model in this context, as we don't have a dependent variable to compare
the different results. We can see however that the k-means, PAM and the hierarchical clustering with average
linkage have given very similar results. Therefore, in this case, there is no best model, however, we think
that the fact that different models have given similar results reinforces, in a way, the results obtained.

## Conclusion

The objective of this project was to find ways to group the set of penguins in a coherent making use of
clusters. When exploring the data, we found that there were several null values that we removed as they did
not represent a significant percentage of the dataset and they could have acted as noise. We also removed the
X (penguin identifier) and year columns, as they did not provide relevant information for the analysis . We
then saw that there was a strong correlation between the remaining variables, so we decided to apply PCA in
order to reduce the dimensionality of the data.

Once we applied PCA, we transformed the dataset to convert it into numerical values. For the partitioning
models, we scaled the dataset and in the case of hierarchical clustering, we normalized the dataset. Once the
dataset was transformed, we applied the different clustering models in order to achieve different
perspectives of the data. In the case of the partitioning models, we applied the k-means, PAM and DBSCAN,
while in the case of hierarchical clustering, we applied the different linkage methods. In each of the models,
we defined the optimal number of clusters using via different methods.

Regarding results, we saw that most of them gave us similar results, specificallty the k-means, PAM, DBSCAN and
some linkage methods of hierarchical clustering. Because of this, We could not say that there was a
better model.

When analyzing the clusters, we noticed that each sex had two clusters assigned to them, which meant that the
clustering process was heavily influenced by this variable. We thought that the clusters could be grouping the
penguins according to their age, but when looking at the variables, we saw that it did not make much sense.

Lastly, we applied the same models but we had removed the sec variable as it was heavily influencing the
clustering and classifying the penguins by sex was superfluous. The results were very similar to previous
analyses, so we could not draw any additional conclusions. Our final conclusion was that the penguins could
be grouped by species and that there are two of them in the dataset, this theory fits with our findings
about there being two different types of penguins, each with differences that were not tied to age.

### Our Learnings

We have learned how complex the process of reaching a conclusion can be when in the context of data
classification. However, we have also learned to apply the different clustering models and, above all, to
interpret the results according to the dataset and its variables.

### References

- [R Documentation] (https://www.rdocumentation.org/): Used to understand some of the functions and packages
  used in our analysis.
- [ChatGPT] (chatgpt.com): Used for help in making sense of the data and translation of some text originally
  written in Spanish.
- [RMD files from class]: Used as a reference for the structure of the document and as examples of
  model implementation.


