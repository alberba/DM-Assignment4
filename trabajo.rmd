---
title: "Assignment 4"
author: "Santiago Rattenbach, Àngel Jiménez, Albert Salom"
date: "21/11/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries, include=FALSE}
# Load the required libraries, without showing warning messages
suppressWarnings({
  suppressPackageStartupMessages({
    library(ggplot2)
    library(GGally)
    library(cluster)
    library(factoextra)
    library(data.table)
    library(fpc)
    
  })
})
```

## The Data

```{r}
data <- read.csv("./penguindata.csv", header=TRUE, stringsAsFactors=TRUE)

# Para las ejecuciones en local de Angel
# data <- read.csv("../DATASETS/penguindata.csv", header=TRUE, stringsAsFactors=TRUE)
str(data)
```

Si hacemos un primer vistazo, podemos observar como tenemos varios valores NAs que deberemos gestionar
posteriormente.

### Independent Variables

The dataset includes measurements taken for penguins in Palmer Archipelago. Variable information:
size (flipper length, body mass, bill dimensions), sex and year.

- **bill_length_mm:** a number denoting bill length (millimeters)
- **bill_depth_mm:** a number denoting bill depth (millimeters)
- **flipper_length_mm:** an integer denoting flipper length (millimeters)
- **body_mass_g:** an integer denoting body mass (grams)
- **sex:** a factor denoting penguin sex (female, male)
- **year:** an integer denoting the study year (2007, 2008, or 2009)

### Summary of the Data

Before start training the model, it is important to analyze each of the independent variables to understand
their values, distribution, and relationship with the target variable.

```{r}
summary(data)
```
We can remove the "x" column as it is not relevant for the analysis.
```{r}
data <- data[, -1]
```

Como podemos observar, tenemos varios valores NA en las variables bill_length_mm, bill_depth_mm, 
flipper_length_mm, body_mass_g y sex. Más tarde los trataremos.

### Missing Value Analysis

To find missing values in the dataset, we can use the `is.na()` function in R. 

```{r}
colSums(is.na(data))
```

Como se puede observar, tenemos varias columnas con valores NA. Al tratarse de pocos valores, podemos borrar 
tranquilamente sin que el modelo se vea afectado.

```{r}
data <- na.omit(data)
summary(data)
```

#### Numerical Variables

- **bill_length_mm:** Este dataset contiene la longitud del pico de los pingüinos en milimetros. Vemos que 
  la longitud del pico de los pinguinos varia entre 32.1 y 59.6 milimetros, con una media de 44mm. No parece 
  haber datos excesivamente inusuales

- **bill_depth_mm:**  La profundidad del pico de los pingüinos también viene en milimetros, en un rango entre 
  13.1mm y 21.5mm, con una media de 17.2mm. De nuevo, no hay ningún dato a resaltar.

- **flipper_length_mm:** La longitud de las aletas de los pinguinos del dataset varia entre 172mm y 231mm, 
  con una media de 201mm. Tampoco parece haber ningun valor atípico.

- **body_mass_g:** En el dataset, podemos observar que el peso de los pinguinos puede variar bastante, esto 
  podría deberse a la edad de los pinguinos, ya que los pinguinos más jóvenes suelen pesar menos. El peso
  de los pinguinos varia entre 2700g y 6300g, con una media de 4202g.

```{r}
# List of numerical variables:
numeric <- c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g')

# Plot the distribution of each numerical variable:
for (n in numeric) {
  print(
    ggplot(data, aes(x = !!sym(n))) +
      geom_histogram(fill = "lightblue", color = "white", bins = 30) +
      labs(title = paste("Distribution of", n), x = n, y = "Frecuency") +
      theme_minimal()
  )
}
```

A primera vista, parece ser que las variables numericas están algo centradas, asemejandose a una distribución
normal. De todas formas, posteriormente realizaremos un análisis de normalidad para confirmar esto.

#### Categorical

```{r}
# List of categorical variables:
categories <- c('year', 'sex')

for (var in categories) {
  print(
    ggplot(data, aes_string(x = var)) +
      geom_bar(fill = "coral1") +
      labs(title = paste("Distribution of", var), x = var, y = "Frecuency") +
      theme_minimal()
  )
}
```

- **sex:** Se puede observar que hay una cantidad similar de pinguinos machos y hembras en el dataset. 
  Esto es importante ya que si hubiera una gran diferencia entre los dos sexos, podría afectar a la 
  precisión del modelo.

- **year:** En el dataset, podemos observar que en cada año se ha hecho el estudio a un número similar de 
  pingüinos. Nuevamente, esto nos beneficiará a la hora de entrenar el modelo.

### Data Correlations

```{r}
data$year <- as.factor(data$year)

ggpairs(data,                 # Data frame
        columns = 1:6,        # Columns
        aes(color = year,     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

Como se puede observar, vemos que el año del estudio no parece tener una gran influencia en las variables
numéricas así como en el sexo de los pinguinos. Esto es importante ya que si hubiera una gran diferencia
entre los años, podría afectar a la precisión del modelo. Por tanto, podemos concluir que el año no es una
variable importante para el modelo y procedemos a eliminarla.

```{r}
data <- data[, -6]
summary(data)
```


#### Numerical Variables

```{r}
numeric_Corr <- data[, c('bill_length_mm', 'bill_depth_mm', 'flipper_length_mm',
                        'body_mass_g')]

ggcorr(numeric_Corr, label = TRUE)
```
 Como se puede observar, existe una gran correlación entre todas las variables numéricas, lo que podría 
 afectar a la precisión del modelo. Para solucionar esto, aplicaremos PCA para así poder reducir la 
 dimensionalidad de los datos.
 
## Model Building

### Convert to Numerical Variables

```{r}
# Convertir las variables categóricas en variables one hot encoding
data_Num <- model.matrix(~ . -1, data = data)

# Convertir el resultado a un data frame
data_Num <- as.data.frame(data_Num)
summary(data_Num)
```

### Scaling the data

Para poder aplicar el clustering de forma correcta, es necesario escalar los datos con una media de 0 y una
desviación estándar de 1. Esto hace que quitemos la barrera de la escala en diferentes variables. 

```{r}
data_Scaled <- scale(data_Num)
summary(data_Scaled)
```

## Clustering models

### PCA 

Para poder ver mejor los resultados, aplicaremos PCA para reducir la dimensionalidad de los datos.

```{r}
data_Scaled.pca<- prcomp(data_Num, center=TRUE, scale=TRUE)
summary(data_Scaled.pca)
```

```{r}
plot(data_Scaled.pca, type="l")
```

Como se puede ver, las 2 componentes principales explican alrededor del 85% de la varianza de los datos. Esto nos
permite reducir la dimensionalidad de los datos sin perder mucha información. Además, a la hora de mostrar
los resultados, podremos visualizarlos en un espacio de 2 dimensiones.

```{r}
data_PCA <- as.data.frame(data_Scaled.pca$x[,1:2])
ggplot(data=data_PCA, aes(x=PC1, y=PC2, color=data$sex)) + geom_point() +
  labs(title="PCA", x="PC1", y="PC2")
```

### Deciding the number of clusters

Una manera de decidir el número de clusters es mediante el silhouette method. Este método nos permite 
determinar el número de clusters que mejor se ajusta a los datos. Para ello, es importante que la puntación	
sea lo más cercana a 1 posible.

Antes de aplicar el silhouette method, es importante ver de manera visual el número de clusters que mejor se
ajusta a los datos. Para ello, aplicaremos el método de "elbow plot".

```{r}
set.seed(123)
# Compute and plot wss for k = 2 to k = 15
k.max <- 15 # Maximal number of clusters
wss <- sapply(1:k.max, 
              function(k){kmeans(data_Scaled, k)$tot.withinss})

plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")
#abline(v = 3, lty =2)
```

Como se puede observar, el rango de 3 a 5 clusters parece ser el más adecuado ya que a partir del 5, apenas 
hay bajado incluso una anormal subida en 6. Para confirmar esto, aplicaremos el silhouette method.

```{r}
cl.kmeans <-kmeans(data_Scaled, 3, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

Como se puede observar, vemos como los 2 primeros clusters tienen una puntuación bastante alta, mientras que 
el tercero tiene una puntuación bastante baja. Esto nos indica que el número de clusters no es el adecuado.

```{r}
cl.kmeans <-kmeans(data_Scaled, 4, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

En este caso, vemos como el silhouette method nos indica que el número de clusters es el adecuado, ya que
una puntuación de media de 0.77, lo cual es bastante bueno. Igualmente, probaremos con 5 clusters para
confirmar que el número de clusters es el adecuado.

```{r}
cl.kmeans <-kmeans(data_Scaled, 5, nstart = 20)
dis <- dist(data_Scaled)^2
sil = silhouette (cl.kmeans$cluster, dis)
summary(sil)
print(mean(sil[,3]))
```

Con 5 clusters vemos como la puntuación no mejora, por lo que es mejor quedarnos con 4 clusters.

## Partitional Clustering

### Clustering using K-means 

```{r}
# Compute the k-means clustering algorithm
kmeans_model <- kmeans(data_Scaled, centers = 4, nstart = 20)
data_PCA <- as.data.table(data_PCA)
cl.kmeans2 <- data_PCA[, km.Clusters := kmeans_model$cluster]

ggplot(cl.kmeans2, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(kmeans_model$cluster)), size = 2, alpha = 0.3) +
theme_minimal()
```

Vemos que los clusters quedan bastante definidos, lo cual es una buena señal. Además, no hay ningún outlier
que perjudique a los propios clusters. Veamos si se pueden sacar conclusiones con algunas variables.

```{r}
data2 <- as.data.table(data)
data2[, km.Clusters := kmeans_model$cluster]
ggplot(data2, aes(x = sex, y = body_mass_g)) +
geom_point(aes(colour = factor(kmeans_model$cluster)), size = 2, alpha = 0.3) +
theme_minimal()
```
 
En este caso, podemos indicar como a las hembras se les asigna el cluster 1 o 4, mientras que a los machos
se les asigna el cluster 2 o 3. Por tanto, el factor del sexo es un factor importante a la hora diferenciar
las muestras. Además se puede apreciar una tendencia ya que el clúster 1 forman parte del grupo de hembras
más pesadas, mientras que el clúster 4 forman parte del grupo de hembras más ligeras. En el caso de los
machos pasa igual, siendo el clúster 2 el que forman parte de los machos más pesados y el clúster 3 los
machos más ligeros.

Observemos a ver si se pueden apreciar mas tendencias con otras variables.

```{r}
ggpairs(data2,        # Columns
        aes(color = factor(kmeans_model$cluster),     # Color by grup (cathegorical)
            alpha = 0.5))     # Transparency
```

En este gráfico se puede sacar bastante información. De primeras, confirmamos lo dicho anteriormente, que
el cluster 1 y 4 son hembras y el cluster 2 y 3 son machos. Por tanto, nos fijaremos en como se relacionan
las otras variables con el sexo de los pingüinos. 

Si nos fijamos en la longitud de la aleta, podemos observar como los clusters 1 y 2, los que pesan mas, 
son los que tienen unalongitud de aleta más larga, mientras que los clusters 3 y 4 son los que tienen una 
longitud de aleta más corta. Podría ayudarnos a diferenciar entre los machos y hembras adultos y los jóvenes.

Sin embargo, si nos fijamos en la profundidad del pico, vemos que la tendencia es inversa a las otras 2.
Según nuestra suposición, esto supondría que los jovenes tienen un pico mayor que los adultos, lo cual no
tiene mucho sentido. Por tanto, los clusters no parecen seguir una división de jovenes y adultos por sexo.
Por tanto, puede ser que el dataset divida por especies de pingüinos. Para ello, más tarde probaremos 
modificando el dataset.

### Clustering around K-Medoids (PAM)

Para conocer el número de clusters óptimo, utilizaremos la función pamk del paquete fpc.

```{r}
pamk <- pamk(distEuclidea)
pamk$nc # Nos muestra el número de clusters óptimo
```

Como podemos apreciar, efectivamente el número de clústers óptimo es 4, el cual es el mismo que hemos
decidido con el k-means.- omsim le se lauc le s

Ahora vamos a probar con este algoritmo, usando 4 clusters, asi.como hemos decidido a
etnemroiret
Probamos primero con la distancia euclidea:

```{r}
distEuclidea <- dist(data_Scaled, method="euclidean")
pam_Euclidean <- pam(distEuclidea, k = 4)
pam_Euclidean
```

```{r}
clusplot(pam_Euclidean)
```

Vemos que los clusters estan definidos igual de bien que con el k-means. Esto se debe a que la distancia 
euclidea es la más adecuada para valores continuas o valores mixtas(continuas y categóricas). Este es 
nuestro caso.

Vamos a ver ahora que obtenemos si usamos la distancia de Manhattan:

```{r}
distManhattan<-dist(data_Scaled, method="manhattan")
# Compute the PAM clustering algorithm
pam_Manhattan <- pam(distManhattan, k = 4)
pam_Manhattan
```

```{r}
clusplot(pam_Manhattan)
```

Como podemos observar, con la distancia de Manhattan los resultados son mucho peores. Se puede apreciar como
las muestras de los clusters están mezcladas, lo cual no es bueno. Por tanto, la distancia euclidea parece 
ser mejor.

### Clustering Density-based

Para el desnity-based clustering, vamos a utilizar el algoritmo DBSCAN. Los parámetros inicialmente son
aleatorios, hasta que encontremos los parámetros que mejor se ajusten a los datos.

```{r}
library(fpc)
dbscan.results <- dbscan(data_Scaled, eps=0.35, MinPts=5) 
dbscan.results
```

```{r}
plot(dbscan.results, data_PCA[, 1:2])
```

Como podemos observar hay demasiados clusters y muchos outliers, que es lo que nos indica la columna de "0", así que probaremos otros parámetros

```{r}
dbscan.results2 <- dbscan(data_Scaled, eps=0.75, MinPts=20) 
dbscan.results2
```
```{r}
plot(dbscan.results2, data_PCA[, 1:2])
```

Sigue habiendo muchos outliers, vamos a intentar mejorarlo

```{r}
dbscan.results3 <- dbscan(data_Scaled, eps=0.85, MinPts=5) 
dbscan.results3
```
```{r}
plot(dbscan.results3, data_PCA[, 1:2])
```

```{r}
plotcluster(data_PCA[, 1:2], dbscan.results3$cluster)
```

Como podemos observar, el número de clusters es el adecuado, ya que apenas hay outliers y los clusters están
bien definidos.

## Hierarchical Clustering

Primero vamos a normalizar los datos para poder aplicar el clustering de forma correcta.

```{r}
# Create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Normalize the data
data_Normalized <- as.data.frame(lapply(data_Num, normalize))
summary(data_Normalized)
```
```{r}
# Calcular la distancia euclidiea
distEuclidea <- dist(data_Normalized[, 1:6], method = "euclidean")
```

```{r}
hclust_single <- hclust(distEuclidea, method = "single")
plot(hclust_single, main = "Dendrograma de Clustering - Single", hang = -1)
```

Como podemos observar, tenemos demasiadas muestras para poder analizar correctamente y se satura el gráfico, vamos a seleccionar un menor número de muestras de forma aleatoria.

```{r}
set.seed(2837)

idx <- sample(1:dim(data_Normalized)[1], 60)  # Seleccionar 60 muestras aleatorias
data_Sample <- data_Normalized[idx, ]  

distEuclideaSample <- dist(data_Sample[, 1:6], method = "euclidean")
```

```{r}
hclust_single2 <- hclust(distEuclideaSample, method = "single")
plot(hclust_single2, main = "Dendrograma de Clustering - Single", cex = 0.6, hang = -1)
```

Ahora procederemos a cortar en 4 clústers, como pensamos que se separan mejor los datos.

```{r}
plot(hclust_single2, main = "Dendrograma de Clustering - Single", cex = 0.6, hang = -1) 

rect.hclust(hclust_single, k=4)
```

```{r}
groups <- cutree(hclust_single, k = 4)

# Añadir los clusters al dataset
data_Sample <- as.data.table(data_Normalized)
data_Sample[, hc.Clusters := groups]
```

Visualizamos los clusters con PCA

```{r}
# Añadir los clusters al dataframe de PCA desde el dataframe original
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```


```{r}
heigth_single<-hclust_single$height # height values
heigth_single
```

```{r}
height <- c(0, heigth_single[-length(heigth_single)]) # vector that has to be substracted from height.cl
round(heigth_single-height, 3) # differences in height, rounded at the 3rd digit
```

```{r}
max(round(heigth_single-height,3)) # the largest increase
```

Este valor es el mayor "salto" en la distancia entre los clusters a lo largo del proceso de agrupación. En el contexto de clustering jerárquico, 
un gran aumento en la altura indica que dos clusters muy distintos (en términos de sus características) se han unido, 
lo que puede ser un buen punto para cortar el dendrograma.

```{r}
which.max(round(heigth_single-height,3)) # the step of the largest increase
```

Como podemos observar con este último valor, el mayor salto se produce en el paso 332, por lo que se determina que el número de clusters óptimo es 2.

We know, however, that these two clusters are probably the result of dividing the penguins between male and
female, which isn't useful at all as that classification is obvious and we are trying to divide the penguins
in a more meaningful way, such as species. Because of this, we are going to try again later but we will
remove the sex column from the dataset, in order to try to divide the penguins in a more meaningful way.



Otros tipos de métodos, average y complete:

```{r}
hclust_avg <- hclust(distEuclidea, method = "average")
plot(hclust_avg, main = "Dendrograma de Clustering - Average", hang = -1)
```

```{r}
groups <- cutree(hclust_avg, k = 4)

# Añadir los clusters al dataset
data_Sample <- as.data.table(data_Normalized)
data_Sample[, hc.Clusters := groups]
```

```{r}
# Añadir los clusters al dataframe de PCA desde el dataframe original
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

As we can see, we end up with very similar results so we can assume that the number of recommended clusters
will be two again. We will try with the complete method now.

```{r}
hclust_complete <- hclust(distEuclidea, method = "complete")
plot(hclust_complete, main = "Dendrograma de Clustering - Complete", hang = -1)
```

```{r}
groups <- cutree(hclust_complete, k = 4)

# Añadir los clusters al dataset
data_Sample <- as.data.table(data_Normalized)
data_Sample[, hc.Clusters := groups]
```

```{r}
# Añadir los clusters al dataframe de PCA desde el dataframe original
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```

Again, we end up with very similar results so we can assume that the number of recommended clusters will be
two again.

Now wee will try again but we will remove the sex column from the dataset.

```{r}
dataNoSex <- data_Normalized[1:4]

# Calcular la distancia euclidea
distEuclidea <- dist(dataNoSex, method = "euclidean")
```

```{r}
hclust_single <- hclust(distEuclidea, method = "single")
plot(hclust_single, main = "Dendrograma de Clustering - Single", hang = -1)
```
```{r}
idx <- sample(1:dim(dataNoSex)[1], 60)  # Seleccionar 60 muestras aleatorias
data_Sample <- dataNoSex[idx, ]  

distEuclideaSample <- dist(data_Sample[, 1:4], method = "euclidean")
```

```{r}
hclust_single2 <- hclust(distEuclideaSample, method = "single")
plot(hclust_single2, main = "Dendrograma de Clustering - Single", cex = 0.6, hang = -1)
```

Ahora procederemos a cortar en 4 clústers, ya que parece ser la mejor forma de separar los datos.

```{r}
plot(hclust_single2, main = "Dendrograma de Clustering - Single", cex = 0.6, hang = -1) 

rect.hclust(hclust_single2, k=4)
```

```{r}
groups <- cutree(hclust_single, k = 4)

# Añadir los clusters al dataset
data_Sample <- as.data.table(dataNoSex)
data_Sample[, hc.Clusters := groups]
```

Visualizamos los clusters con PCA

```{r}
# Añadir los clusters al dataframe de PCA desde el dataframe original
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```
No entiendo por qué da así el dibujo

```{r}
heigth_single<-hclust_single$height # height values
height <- c(0, heigth_single[-length(heigth_single)]) # vector that has to be substracted from height.cl

which.max(round(heigth_single-height,3)) # the step of the largest increase
```
Como podemos observar con este último valor, el mayor salto se produce en el paso 331, por lo que se determina que el número de clusters óptimo es 3.

Ahora probemos con el método average y el complete.

```{r}
hclust_average <- hclust(distEuclidea, method = "average")
plot(hclust_average, main = "Dendrograma de Clustering - Average", hang = -1)
```
```{r}
idx <- sample(1:dim(dataNoSex)[1], 60)  # Seleccionar 60 muestras aleatorias
data_Sample <- dataNoSex[idx, ]  

distEuclideaSample <- dist(data_Sample[, 1:4], method = "euclidean")
```

```{r}
groups <- cutree(hclust_average, k = 4)

# Añadir los clusters al dataset
data_Sample <- as.data.table(dataNoSex)
data_Sample[, hc.Clusters := groups]
```

Visualizamos los clusters con PCA

```{r}
# Añadir los clusters al dataframe de PCA desde el dataframe original
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```
No entiendo por qué da así el dibujo

```{r}
heigth_average<-hclust_average$height # height values
height <- c(0, heigth_average[-length(heigth_average)]) # vector that has to be substracted from height.cl

which.max(round(heigth_average-height,3)) # the step of the largest increase
```
Según esto conviene 2 clusters

COMPLETE

```{r}
hclust_complete <- hclust(distEuclidea, method = "complete")
plot(hclust_complete, main = "Dendrograma de Clustering - Complete", hang = -1)
```
```{r}
idx <- sample(1:dim(dataNoSex)[1], 60)  # Seleccionar 60 muestras aleatorias
data_Sample <- dataNoSex[idx, ]  

distEuclideaSample <- dist(data_Sample[, 1:4], method = "euclidean")
```

```{r}
groups <- cutree(hclust_complete, k = 4)

# Añadir los clusters al dataset
data_Sample <- as.data.table(dataNoSex)
data_Sample[, hc.Clusters := groups]
```

Visualizamos los clusters con PCA

```{r}
# Añadir los clusters al dataframe de PCA desde el dataframe original
cl.hclust <- data_PCA[, hc.Clusters := groups]

ggplot(cl.hclust, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = factor(cl.hclust$hc.Clusters)), size = 2, alpha = 0.3) +
theme_minimal()
```
No entiendo por qué da así el dibujo

```{r}
heigth_complete<-hclust_complete$height # height values
height <- c(0, heigth_complete[-length(heigth_complete)]) # vector that has to be substracted from height.cl

which.max(round(heigth_complete-height,3)) # the step of the largest increase
```


## Conclusion



### Choosing the best model



### Our Learnings



### References

- [R Documentation] (https://www.rdocumentation.org/): Used to understand some of the functions and packages
  used in our analysis.
- [ChatGPT] (chatgpt.com): Used for help in making sense of the data, translation of some text originally
  written in Spanish and removal of certain warnings generated by library functions.
- [RMD files from class]: Used as a reference for the structure of the document and as examples of
  model implementation.
